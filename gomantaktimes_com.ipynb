{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime,timedelta\n",
    "from datetime import date\n",
    "import re\n",
    "import sys\n",
    "import urllib, urllib.request, urllib.parse\n",
    "import random\n",
    "from scrawl import *\n",
    "\n",
    "# Date and time\n",
    "start_time = time.time()\n",
    "current_time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "created_on = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# client_id = sys.argv[1]\n",
    "client_id = '5f69d22ef472d6646f577fa6'  # Europe\n",
    "site = 'gomantaktimes_com'\n",
    "c = Crawl()  # creating object\n",
    "cl_data = dashboard['core_web_india']\n",
    "# create directories to store logs.\n",
    "log_path = c.create_directories(project_path, client_id, site)\n",
    "\n",
    "# create image directories\n",
    "image_directory = c.create_image_directories(project_path)\n",
    "\n",
    "# logger\n",
    "logger = log_func(log_path, created_on, current_time)\n",
    "logger.info(\"Process Started ...\\n\")\n",
    "\n",
    "# initialize variables\n",
    "skipped_due_to_headline = 0\n",
    "skipped_due_to_content = 0\n",
    "skipped_due_to_date = 0\n",
    "missing_overall_tonality = 0\n",
    "no_of_data = 0\n",
    "duplicate_data = 0   \n",
    "unable_to_fetch_url = 0\n",
    "unable_to_fetch_rss_page = 0\n",
    "publish_source = 'gomantaktimes.com'\n",
    "country = 'India'\n",
    "language = 'English'\n",
    "images_path = []\n",
    "\n",
    "rss_pages = c.download_page('https://www.gomantaktimes.com/rss-feed/')\n",
    "\n",
    "rss_pages = c.scrap('<article\\s*id=\"node-73\"(.*?)</article>', rss_pages)\n",
    "#print(rss_pages)\n",
    "\n",
    "for _ in rss_pages.split('href=')[1:]:\n",
    "    \n",
    "    rss_url = c.scrap('\"(.*?)\"', _)\n",
    "    rss_url='https://www.gomantaktimes.com'+rss_url\n",
    "    url = c.download_page(rss_url)\n",
    "    \n",
    "    if url.startswith('Unable to fetch'):\n",
    "        logger.info(url) # writes error message with error code\n",
    "        unable_to_fetch_rss_page += 1\n",
    "        continue \n",
    "    for i in url.split('<item>')[1:]:\n",
    "\n",
    "        # source_link\n",
    "        source_link = c.scrap('<link>(.*?)</link>', i)\n",
    "        \n",
    "        # handle duplicates\n",
    "        source_link_query = {'source_link':source_link}\n",
    "        dic = cl_data.find_one(source_link_query,{'source_link': 1}) \n",
    "        if dic:\n",
    "            duplicate_data += 1\n",
    "            continue         \n",
    "        \n",
    "        time.sleep(random.randint(1,3))\n",
    "        \n",
    "        page = c.download_page(source_link)\n",
    "        if page.startswith('Unable to fetch'):\n",
    "                logger.info(page) # writes error message with error code\n",
    "                unable_to_fetch_url += 1\n",
    "                continue  \n",
    "       \n",
    "        # source_headline\n",
    "        source_headline = c.scrap('<h1>(.*?)</h1>', page)\n",
    "        \n",
    "        # skip if headline not found\n",
    "        if not source_headline:\n",
    "            logger.info(f'Skipping due to headline {source_link}\\n')\n",
    "            skipped_due_to_headline += 1\n",
    "            continue\n",
    "\n",
    "        logger.info(f'Fetching {source_link}\\n')\n",
    "        \n",
    "         # Date and time\n",
    "        pub_date, publish_time = '', ''\n",
    "\n",
    "        try:\n",
    "            date_time_str = c.scrap('<pubDate>\\w+,\\s+(.*?) \\+', i)\n",
    "            date_time_obj = datetime.strptime(date_time_str, '%d %b %Y %H:%M:%S')\n",
    "            # February282021000000\n",
    "            # There is no time difference between India and Sri Lanka\n",
    "            ist_date_time = date_time_obj - timedelta(hours = 0,minutes = 0)  \n",
    "            ist_date_time = ist_date_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            pub_date = ist_date_time[:10]\n",
    "            publish_time = ist_date_time[11:]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # skip null date\n",
    "        if not pub_date:\n",
    "            logger.info(f'Skipping due to date {source_link}\\n')\n",
    "            skipped_due_to_date += 1\n",
    "            continue\n",
    "\n",
    "        # break if date is not today's date\n",
    "        if pub_date != created_on:\n",
    "            break    \n",
    "                \n",
    "         # journalist   \n",
    "        journalist = c.scrap('<div\\s*class=\"writer_details\">(.*?)</div>', page)\n",
    "        journalist = re.sub('<.*?>', '', journalist, flags=re.S)\n",
    "        if not journalist: journalist = 'NA'\n",
    "\n",
    "        \n",
    "         # source_content          \n",
    "        source_content = c.scrap('<div\\s*class=\"panel-pane\\s*pane-node-body\\s*news-summary\\s*detail-title\\s*text-style\">(.*?)<div class=\"panel-pane pane-views pane-search-view news-story-detail-page-content-carousel\"', page)\n",
    "        #source_content = re.sub('<figcaption\\s*class=\"caption\">(.*?)</figcaption>','',source_content,flags=re.S) \n",
    "        source_content = c.strip_html(source_content)\n",
    "        #source_content = re.sub('&amp;','&',source_content,flags=re.S)\n",
    "  \n",
    "        \n",
    "         # current date and time \n",
    "        harvest_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "        # temp link\n",
    "        temp_link = source_link\n",
    "\n",
    "        # headline and content \n",
    "        headline = source_headline\n",
    "        content = source_content\n",
    "\n",
    "        # overall_tonality\n",
    "        overall_tonality = ''\n",
    "\n",
    "        # word count\n",
    "        word_count = len((source_headline + ' ' + source_content).split())\n",
    "\n",
    "        html_content = ''\n",
    "\n",
    "        # image_urls\n",
    "        image_urls = []\n",
    "\n",
    "        # storing the above data in a dictionary\n",
    "        clientdata ={\n",
    "                        \"client_master\" : client_id, \n",
    "                        \"articleid\":client_id,\n",
    "                        \"medium\":'Web' ,\n",
    "                        \"searchkeyword\":[],\n",
    "                        \"entityname\" : [] ,\n",
    "                        \"process_flage\":\"1\",\n",
    "                        \"na_flage\":\"0\",\n",
    "                        \"na_reason\":\"\",\n",
    "                        \"qc_by\":\"\",\n",
    "                        \"qc_on\":\"\",\n",
    "                        \"location\":\"\",\n",
    "                        \"spokeperson\":\"\",\n",
    "                        \"quota\":\"\",\n",
    "                        \"overall_topics\":\"\",\n",
    "                        \"person\":\"\",\n",
    "                        \"overall_entites\":\"\",\n",
    "                        \"overall_tonality\": overall_tonality,\n",
    "                        \"overall_wordcount\":word_count,\n",
    "                        \"article_subjectivity\":\"\",\n",
    "                        \"article_summary\":\"\",\n",
    "                        \"pub_date\":pub_date,\n",
    "                        \"publish_time\":publish_time,\n",
    "                        \"harvest_time\":harvest_time,\n",
    "                        \"temp_link\":temp_link,\n",
    "                        \"publish_source\": publish_source,\n",
    "                        \"programme\":'null',\n",
    "                        \"feed_class\":\"News\",\n",
    "                        \"publishing_platform\":\"\",\n",
    "                        \"klout_score\":\"\",\n",
    "                        \"journalist\":journalist,\n",
    "                        \"headline\":headline,\n",
    "                        \"content\":content,\n",
    "                        \"source_headline\":source_headline,\n",
    "                        \"source_content\":source_content,\n",
    "                        \"language\":language,\n",
    "                        \"presence\":'null',\n",
    "                        \"clip_type\":'null',\n",
    "                        \"prog_slot\":'null',\n",
    "                        \"op_ed\":'0',\n",
    "                        \"location_mention\":'',\n",
    "                        \"source_link\":source_link,\n",
    "                        \"author_contact\":'',\n",
    "                        \"author_emailid\":'',\n",
    "                        \"author_url\":'',\n",
    "                        \"city\":'',\n",
    "                        \"state\":'',\n",
    "                        \"country\":country,\n",
    "                        \"source\":publish_source,\n",
    "                        \"foot_fall\":'',\n",
    "                        \"created_on\":created_on,\n",
    "                        \"active\":'1',\n",
    "                        'crawl_flag':2,\n",
    "                        \"images_path\":images_path,\n",
    "                        \"html_content\":html_content\n",
    "                    } \n",
    "\n",
    "        cl_data.insert_one(clientdata)  \n",
    "        no_of_data += 1\n",
    "        \n",
    "logger.info('Iteration complete\\n')   \n",
    "\n",
    "logger.info(f'Number of data: {no_of_data}\\n')\n",
    "logger.info(f'Duplicate data: {duplicate_data}\\n')\n",
    "logger.info(f'Unable to fetch rss url: {unable_to_fetch_rss_page}\\n')\n",
    "logger.info(f'Unable to fetch article url: {unable_to_fetch_url}\\n')\n",
    "logger.info(f'Skipped due to headline: {skipped_due_to_headline}\\n')\n",
    "logger.info(f'Skipped due to content: {skipped_due_to_content}\\n')\n",
    "logger.info(f'Skipped due to date: {skipped_due_to_date}\\n')\n",
    "logger.info(f'Processing finished in {time.time() - start_time} seconds.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nation\n",
    "\n",
    "World\n",
    "\n",
    "STATES\n",
    "Tamil Nadu\n",
    "\n",
    "Telangana\n",
    "\n",
    "Karnataka\n",
    "\n",
    "Andhra Pradesh\n",
    "\n",
    "Kerala\n",
    "\n",
    "Odisha\n",
    "\n",
    "CITIES\n",
    "Chennai\n",
    "\n",
    "Bengaluru\n",
    "\n",
    "Hyderabad\n",
    "\n",
    "Thiruvananthapuram\n",
    "\n",
    "Kochi\n",
    "\n",
    "Delhi\n",
    "\n",
    "Mumbai\n",
    "\n",
    "Kolkata\n",
    "\n",
    "Ahmedabad\n",
    "\n",
    "BUSINESS\n",
    "Marketing\n",
    "\n",
    "SPORT\n",
    "Cricket\n",
    "\n",
    "Scorecard\n",
    "\n",
    "Tennis\n",
    "\n",
    "Football\n",
    "\n",
    "Fifa U-17 World Cup 2017\n",
    "\n",
    "Asian Games\n",
    "\n",
    "Other\n",
    "\n",
    "Commonwealth Games 2018\n",
    "\n",
    "IPL\n",
    "\n",
    "News\n",
    "\n",
    "Schedule\n",
    "\n",
    "Express@IPL\n",
    "\n",
    "ENTERTAINMENT\n",
    "English\n",
    "\n",
    "Hindi\n",
    "\n",
    "Kannada\n",
    "\n",
    "Malayalam\n",
    "\n",
    "Tamil\n",
    "\n",
    "Telugu\n",
    "\n",
    "Review\n",
    "\n",
    "GALLERIES\n",
    "Nation\n",
    "\n",
    "World\n",
    "\n",
    "Sport\n",
    "\n",
    "Other\n",
    "\n",
    "VIDEOS\n",
    "Nation\n",
    "\n",
    "World\n",
    "\n",
    "Other\n",
    "\n",
    "Sport\n",
    "\n",
    "ThinkEdu\n",
    "\n",
    "Good News\n",
    "\n",
    "Specials\n",
    "\n",
    "OPINIONS\n",
    "Editorials\n",
    "\n",
    "Columns\n",
    "\n",
    "T J S George\n",
    "\n",
    "S Gurumurthy\n",
    "\n",
    "Ravi Shankar\n",
    "\n",
    "Shankkar Aiyar\n",
    "\n",
    "Shampa Dhar-Kamath\n",
    "\n",
    "V Sudarshan\n",
    "\n",
    "Soli J Sorabjee\n",
    "\n",
    "Karamathullah K Ghori\n",
    "\n",
    "Debate\n",
    "\n",
    "LIFESTYLE\n",
    "Tech\n",
    "\n",
    "Health\n",
    "\n",
    "Travel\n",
    "\n",
    "Food\n",
    "\n",
    "Books\n",
    "\n",
    "Spirituality\n",
    "\n",
    "Astrology\n",
    "\n",
    "Good News\n",
    "\n",
    "ELECTIONS\n",
    "Andhra Elections\n",
    "\n",
    "Odisha Elections\n",
    "\n",
    "Sikkim Elections\n",
    "\n",
    "Arunachal Elections\n",
    "\n",
    "Haryana Elections\n",
    "\n",
    "Maharashtra Elections\n",
    "\n",
    "Delhi Elections\n",
    "\n",
    "Bihar Elections\n",
    "\n",
    "Tamil Nadu Elections\n",
    "\n",
    "Interviews\n",
    "\n",
    "Constituency Watch\n",
    "\n",
    "Assam Elections\n",
    "\n",
    "Puducherry Elections\n",
    "\n",
    "Punjab Elections\n",
    "\n",
    "EDUCATION\n",
    "Edex\n",
    "\n",
    "Auto\n",
    "\n",
    "Indulge\n",
    "\n",
    "MAGAZINE\n",
    "Voices\n",
    "\n",
    "The Sunday Standard\n",
    "\n",
    "E-Paper\n",
    "\n",
    "Resources\n",
    "\n",
    "PRABHU CHAWLA\n",
    "Columns\n",
    "\n",
    "Ask Prabhu\n",
    "\n",
    "Top News\n",
    "\n",
    "AUDIOS\n",
    "New Articles\n",
    "\n",
    "Search\n",
    "\n",
    "Glance\n",
    "\n",
    "Coronavirus\n",
    "\n",
    "Express Connect\n",
    "\n",
    "2020 WITH TNIE\n",
    "Recap 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
