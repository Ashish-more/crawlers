{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suited-precipitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.politico.eu/article/josep-borrell-eu-doesnt-have-resources-fight-disinfo-china/\n",
      "Borrell: EU doesn't have resources to fight disinformation from China\n",
      "s aa-dataset-d1\">\n",
      "\n",
      "\t\t\t\t\tPolitico Pro\n",
      "https://www.newyorker.com/news/a-reporter-at-large/china-xinjiang-prison-state-uighur-detention-camps-prisoner-testimony\n",
      "Inside Xinjiang’s Prison State\n",
      "\">\n",
      "https://www.barrons.com/articles/bitcoin-mining-in-xinjiang-china-could-be-a-red-flag-for-regulators-51613764881\n",
      "Bitcoin Mining Is Big in China. Why Investors Should Worry.\n",
      ".app/lib/module/barronsBanner\" data-module-zone=\"barrons_ie_banner\" class=\"zonedModule\">\n",
      "https://americancompass.org/the-commons/magical-thinking-on-china-and-trade/\n",
      "Magical Thinking on China and Trade - American Compass\n",
      "-copy sserif main-article-wrapper standard-commons-entry cf\">\n",
      "            In the Wall Street Journal today, former Treasury Secretary Hank Paulson gives a rundown of the generic case for &#8220;How American Free Trade Can Outdo China.&#8221; Three things jump out:\n",
      "1. He leads with the point that, &#8220;America’s economic prosperity and the effectiveness of our political system and global leadership are rooted in domestic economic strength,&#8221; and his first recommendation is &#8220;an economic recovery program that invests in core technologies such as telecommunications and advanced computing.&#8221; That&#8217;s a welcome acknowledgment of the need for policymakers to identify critical industries and develop policies that will channel investment accordingly.\n",
      "2. Paulson frames the public debate thusly: &#8220;Winning over trade skeptics on both left and right will require Mr. Biden to explain how helping companies compete in global markets benefits American workers and families.&#8221; I&#8217;ve yet to meet a trade skeptic who doesn&#8217;t think that helping American companies compete in global markets benefits American workers and families. Winning over trade skeptics will require acknowledging the actual drawbacks of free trade with authoritarian mercantilists and explaining how policy reforms can address this. Which brings us to&#8230;\n",
      "3. Paulson&#8217;s suggestion for dealing with China: &#8220;We should initiate new bilateral talks that open up China to our export industries, such as environmental goods and precision machinery, without forgetting to strengthen intellectual-property protections and eliminate unfair trade practices.&#8221; Let&#8217;s also ask for ponies. Flying ponies. We should be thankful for the admission that China is not open to our export industries, that it does not protect our intellectual property, and that its trade practices are unfair. But what is the plan for changing that? Asking nicely?\n",
      "The thing about trade negotiations is that they are negotiations. Once must have something to offer, or else have a source of leverage to exercise. The U.S. has a great deal of potential leverage over China and it was the Trump administration&#8217;s strategy to develop and use it. Whether they did that well or not, we can debate at great length. But while Paulson clearly considers the Trump strategy a failure, he offers none of his own. Unilaterally disarming from trade conflict on behalf of open markets, and then making empty demands, is not a plan.\n",
      "    Return to the Commons\n",
      "https://news.gallup.com/poll/331082/china-russia-images-hit-historic-lows.aspx\n",
      "China, Russia Images in U.S. Hit Historic Lows\n",
      "s-videos.aspx\">\n",
      "\n",
      "CliftonStrengths Insights\n",
      "Our proven strategies for successful strengths-based development.\n",
      "\n",
      "Education Insights\n",
      "Our applicable and actionable best practices for education leaders.\n",
      "\n",
      "Gallup News\n",
      "Learn the attitudes & behaviors of the world’s 7 billion citizens at news.gallup.com\n",
      "\n",
      "                Shop\n",
      "\n",
      "                About Us\n",
      "Locations\n",
      "Careers\n",
      "Contact Us\n",
      "Subscribe\n",
      "https://www.wsj.com/articles/who-investigators-to-scrap-interim-report-on-probe-of-covid-19-origins-11614865067\n",
      "WSJ News Exclusive | WHO Investigators to Scrap Plans for Interim Report on Probe of Covid-19 Origins\n",
      ".app/lib/webui/modules/wsj/skip\" data-module-zone=\"skip\" class=\"zonedModule\">\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os \n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime,timedelta\n",
    "from datetime import date\n",
    "import re\n",
    "import sys\n",
    "import urllib, urllib.request, urllib.parse\n",
    "import random\n",
    "from scrawl import *\n",
    "    \n",
    "# Date and time\n",
    "start_time = time.time()\n",
    "current_time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "created_on = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# client_id = sys.argv[1]\n",
    "client_id = '5f69d22ef472d6646f577fa6'  # Europe\n",
    "site = 'hoover_org_publications'\n",
    "c = Crawl()  # creating object\n",
    "\n",
    "# create directories to store logs.\n",
    "log_path = c.create_directories(project_path, client_id, site)\n",
    "\n",
    "# create image directories\n",
    "image_directory = c.create_image_directories(project_path)\n",
    "\n",
    "# logger\n",
    "logger = log_func(log_path, created_on, current_time)\n",
    "logger.info(\"Process Started ...\\n\")\n",
    "\n",
    "# initialize variables\n",
    "skipped_due_to_headline = 0\n",
    "skipped_due_to_content = 0\n",
    "skipped_due_to_date = 0\n",
    "missing_overall_tonality = 0\n",
    "no_of_data = 0\n",
    "duplicate_data = 0  \n",
    "unable_to_fetch_article_url = 0\n",
    "publish_source = 'hoover.org'\n",
    "country = 'United States'\n",
    "language = 'English'\n",
    "images_path = []\n",
    "\n",
    "# archive page\n",
    "cat_page = c.download_page('https://www.hoover.org/publications/china-global-sharp-power-weekly-alert')\n",
    "cat_page = c.scrap('<table class=\"views-view-grid(.*?)</tbody>',cat_page)\n",
    "for i in cat_page.split('<div class=\"views-field views-field-title\">')[1:]:\n",
    "    \n",
    "    # source_link\n",
    "    source_link = c.scrap('<a href=\"(.*?)\"', i)\n",
    "    print(source_link)\n",
    "    \n",
    "    # handle duplicates\n",
    "    source_link_query = {'source_link':source_link}\n",
    "    dic = cl_data.find_one(source_link_query,{'source_link': 1}) \n",
    "    if dic:\n",
    "        duplicate_data += 1\n",
    "        continue\n",
    "        \n",
    "    time.sleep(random.randint(1,3))\n",
    "    page = c.download_page(source_link)\n",
    "    if page.startswith('Unable to fetch'):\n",
    "        logger.info(page) # writes error message with error code\n",
    "        unable_to_fetch_article_url += 1\n",
    "        continue \n",
    "    \n",
    "    # source_headline\n",
    "    source_headline = c.scrap('<meta\\s*property=\"og:title\" content=\"(.*?)\"', page)\n",
    "    source_headline = source_headline.replace(\"&#8217;\",\"'\")\n",
    "    print(source_headline)\n",
    "    \n",
    "\n",
    "    # skip if headline not found\n",
    "    if not source_headline:\n",
    "        logger.info(f'Skipping due to headline {source_link}\\n')\n",
    "        skipped_due_to_headline += 1\n",
    "        continue\n",
    "\n",
    "    # Date and time\n",
    "    pub_date, publish_time = '', ''\n",
    "\n",
    "    try:\n",
    "        date_time_str = c.scrap('\"datePublished\":\"(.*?)\"', page)  \n",
    "        date_time_str = date_time_str.replace(\"Z\",\"\")\n",
    "        date_time_str = date_time_str.replace(\".000\",\"\")\n",
    "        date_time_str = date_time_str.replace(\"-05:00\",\"\")\n",
    "        date_time_str = date_time_str.replace(\"+00:00\",\"\")\n",
    "        if 'T' not in date_time_str:\n",
    "            date_time_str = date_time_str +'T' + '00:00:00'\n",
    "        \n",
    "        date_time_obj = datetime.strptime(date_time_str, '%Y-%m-%dT%H:%M:%S')\n",
    "        ist_date_time = date_time_obj + timedelta(hours = 0,minutes = 0)  # utc time to ist\n",
    "        ist_date_time = ist_date_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        pub_date = ist_date_time[:10]\n",
    "        publish_time = ist_date_time[11:]\n",
    "    except:\n",
    "        pass\n",
    "#     print(pub_date)\n",
    "#     print(publish_time)\n",
    "#     continue\n",
    "    # skip null date\n",
    "    if not pub_date:\n",
    "        logger.info(f'Skipping due to date {source_link}\\n')\n",
    "        skipped_due_to_date += 1\n",
    "        continue\n",
    "\n",
    "    # break if date is not today's date\n",
    "#     if pub_date != created_on:\n",
    "#         break    \n",
    "\n",
    "    logger.info(f'Fetching {source_link}\\n')\n",
    "        \n",
    "    # source_content          \n",
    "    source_content = c.scrap('<div\\s*class=\".*?article(.*?)<div', page)\n",
    "    source_content = c.strip_html(source_content)\n",
    "    print(source_content)\n",
    "    continue\n",
    "\n",
    "    # skip if content not found\n",
    "    if not source_content:\n",
    "        logger.info(f'Skipping due to content {source_link}\\n')\n",
    "        skipped_due_to_content += 1\n",
    "        continue\n",
    "\n",
    "    # journalist\n",
    "    journalist = ''\n",
    "    if not journalist: journalist = 'NA'\n",
    "\n",
    "    # current date and time \n",
    "    harvest_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "    # temp link\n",
    "    temp_link = source_link\n",
    "\n",
    "    # headline and content \n",
    "    headline = source_headline\n",
    "    content = source_content\n",
    "\n",
    "    # overall_tonality\n",
    "    overall_tonality = ''\n",
    "\n",
    "    # word count\n",
    "    word_count = len((source_headline + ' ' + source_content).split())\n",
    "\n",
    "    html_content = ''\n",
    "\n",
    "    # image_urls\n",
    "    image_urls = []\n",
    "    \n",
    "    # storing the above data in a dictionary\n",
    "    clientdata ={\n",
    "                    \"client_master\" : client_id, \n",
    "                    \"articleid\":client_id,\n",
    "                    \"medium\":'Web' ,\n",
    "                    \"searchkeyword\":[],\n",
    "                    \"entityname\" : [] ,\n",
    "                    \"process_flage\":\"1\",\n",
    "                    \"na_flage\":\"0\",\n",
    "                    \"na_reason\":\"\",\n",
    "                    \"qc_by\":\"\",\n",
    "                    \"qc_on\":\"\",\n",
    "                    \"location\":\"\",\n",
    "                    \"spokeperson\":\"\",\n",
    "                    \"quota\":\"\",\n",
    "                    \"overall_topics\":\"\",\n",
    "                    \"person\":\"\",\n",
    "                    \"overall_entites\":\"\",\n",
    "                    \"overall_tonality\": overall_tonality,\n",
    "                    \"overall_wordcount\":word_count,\n",
    "                    \"article_subjectivity\":\"\",\n",
    "                    \"article_summary\":\"\",\n",
    "                    \"pub_date\":pub_date,\n",
    "                    \"publish_time\":publish_time,\n",
    "                    \"harvest_time\":harvest_time,\n",
    "                    \"temp_link\":temp_link,\n",
    "                    \"publish_source\": publish_source,\n",
    "                    \"programme\":'null',\n",
    "                    \"feed_class\":\"News\",\n",
    "                    \"publishing_platform\":\"\",\n",
    "                    \"klout_score\":\"\",\n",
    "                    \"journalist\":journalist,\n",
    "                    \"headline\":headline,\n",
    "                    \"content\":content,\n",
    "                    \"source_headline\":source_headline,\n",
    "                    \"source_content\":source_content,\n",
    "                    \"language\":language,\n",
    "                    \"presence\":'null',\n",
    "                    \"clip_type\":'null',\n",
    "                    \"prog_slot\":'null',\n",
    "                    \"op_ed\":'0',\n",
    "                    \"location_mention\":'',\n",
    "                    \"source_link\":source_link,\n",
    "                    \"author_contact\":'',\n",
    "                    \"author_emailid\":'',\n",
    "                    \"author_url\":'',\n",
    "                    \"city\":'',\n",
    "                    \"state\":'',\n",
    "                    \"country\":country,\n",
    "                    \"source\":publish_source,\n",
    "                    \"foot_fall\":'',\n",
    "                    \"created_on\":created_on,\n",
    "                    \"active\":'1',\n",
    "                    'crawl_flag':2,\n",
    "                    \"images_path\":images_path,\n",
    "                    \"html_content\":html_content\n",
    "                } \n",
    "\n",
    "    cl_data.insert_one(clientdata)  # get object id and insert data\n",
    "    no_of_data += 1\n",
    "\n",
    "\n",
    "logger.info('Iteration complete\\n')   \n",
    "\n",
    "logger.info(f'Number of data: {no_of_data}\\n')\n",
    "logger.info(f'Duplicate data: {duplicate_data}\\n')\n",
    "logger.info(f'Unable to article fetch url: {unable_to_fetch_article_url}\\n')\n",
    "logger.info(f'Skipped due to headline: {skipped_due_to_headline}\\n')\n",
    "logger.info(f'Skipped due to content: {skipped_due_to_content}\\n')\n",
    "logger.info(f'Skipped due to date: {skipped_due_to_date}\\n')\n",
    "logger.info(f'Processing finished in {time.time() - start_time} seconds.\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-mason",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-kitty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
