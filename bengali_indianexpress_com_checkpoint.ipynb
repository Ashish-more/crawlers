import json
import os 
import requests
import time
from datetime import datetime,timedelta
from datetime import date
import re
import sys
import urllib, urllib.request, urllib.parse
import random
from scrawl import *
    
# Date and time
start_time = time.time()
current_time = datetime.now().strftime("%H-%M-%S")
created_on = date.today().strftime("%Y-%m-%d")

# client_id = sys.argv[1]
client_id = '5f69d22ef472d6646f577fa6'  # Europe
site = 'bengali_indianexpress_com'
c = Crawl()  # creating object

# create directories to store logs.
log_path = c.create_directories(project_path, client_id, site)

# create image directories
image_directory = c.create_image_directories(project_path)

# logger
logger = log_func(log_path, created_on, current_time)
logger.info("Process Started ...\n")

# initialize variables
skipped_due_to_headline = 0
skipped_due_to_content = 0
skipped_due_to_date = 0
missing_overall_tonality = 0
no_of_data = 0
duplicate_data = 0  
unable_to_fetch_url = 0
unable_to_fetch_rss_page = 0
publish_source = 'bengali.indianexpress.com'
country = 'India'
language = 'Bengali'
images_path = []

cl_data = []


rss_pages = c.download_page('https://bengali.indianexpress.com/feed/')
for i in rss_pages.split('<item>')[1:]:
    source_link = c.scrap('<link>(.*?)</link>', i)
    
    time.sleep(random.randint(1,3))
    
    page = c.download_page(source_link)
    if page.startswith('Unable to fetch'):
                logger.info(page) # writes error message with error code
                unable_to_fetch_url += 1
                continue
    
    source_headline = c.scrap('<title>(.*?)</title>', i)
    
    
    
    if not source_headline:
        logger.info(f'Skipping due to headline {source_link}\n')
        skipped_due_to_headline += 1
        continue
    
    logger.info(f'Fetching {source_link}\n')
    
    source_content = c.scrap('<content:encoded>(.*?)</content:encoded>',i)
    source_content = source_content.replace("<![CDATA[","").replace("]]>","")
    source_content = c.strip_html(source_content)
   
    
    
    if not source_content:
        logger.info(f'Skipping due to content {source_link}\n')
        skipped_due_to_content += 1
        continue
        
    journalist =  ''
    if not journalist: journalist = 'NA'
    pub_date, publish_time = '', ''            
            
    #try:
    date_time_str = c.scrap('<pubDate>(.*?)\+', i)
    date_time_str = re.sub('\s|,|:','',date_time_str,flags=re.S)
    date_time_obj = datetime.strptime(date_time_str, '%a%d%b%Y%H%M%S')
    ist_date_time = date_time_obj + timedelta(hours = 5,minutes = 30)  
    ist_date_time = ist_date_time.strftime('%Y-%m-%d %H:%M:%S')  
    pub_date = ist_date_time[:10]
    publish_time = ist_date_time[11:]
   
            
   # except:
        #pass
    
        
    if not pub_date:
        logger.info(f'Skipping due to date {source_link}\n')
        skipped_due_to_date += 1
        continue
        
    
        
    
        
    # current date and time 
    harvest_time = datetime.now().strftime("%H:%M:%S")
    # temp link
    temp_link = source_link
    # headline and content 
    headline = source_headline
    content = source_content
    # overall_tonality
    overall_tonality = ''
    # word count
    word_count = len((source_headline + ' ' + source_content).split())
    html_content = ''
    # image_urls
    image_urls = []
    
    clientdata = {
            'source_link': source_link,
            'source_headline': source_headline,
            'source_content': source_content,
            'journalist': journalist,
            'pub_date_time': ist_date_time
        }
    cl_data.append(clientdata)
    no_of_data += 1
    
with open(f'{site}.json', 'w') as f:
    f.write(json.dumps(cl_data, indent=6))
logger.info('Iteration complete\n')
logger.info(f'Number of data: {no_of_data}\n')
logger.info(f'Duplicate data: {duplicate_data}\n')
logger.info(f'Unable to fetch rss url: {unable_to_fetch_rss_page}\n')
logger.info(f'Unable to fetch article url: {unable_to_fetch_url}\n')
logger.info(f'Skipped due to headline: {skipped_due_to_headline}\n')
logger.info(f'Skipped due to content: {skipped_due_to_content}\n')
logger.info(f'Skipped due to date: {skipped_due_to_date}\n')
logger.info(f'Processing finished in {time.time() - start_time} seconds.\n')